{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning \n",
    "## Image Classification using Shallow ML Algorithms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Datasets:\n",
    "1. The CIFAR-10 dataset - the dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "2. Fashion MNIST - a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/kx8r1rpd7lbg37j10fbnmvnr0000gn/T/ipykernel_27166/4092325853.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dataframe_image as dfi\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Configuration & Function Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": \"fashionmnist\",\n",
    "    \"n_clusters\": 100,\n",
    "    \"param_decision_tree\": {\n",
    "        \"n_estimators\": [50, 150, 200, 500], #number of trees\n",
    "        \"max_depth\": [5, 10, 7, 25, 50], #depth of the trees\n",
    "        \"min_samples_split\": [2, 4, 7, 9, 10], #min num. of samples to split\n",
    "        \"min_samples_leaf\": [1, 3, 5] #min num. of samples in a leaf node\n",
    "    },\n",
    "    \"param_svc\": {\n",
    "        \"kernel\": ['rbf', 'linear'] # hyperplane type\n",
    "    },\n",
    "    \"knearest_neighbor_param\": {\n",
    "        \"n_neighbors\": [3, 5, 7], #num. of neighboors\n",
    "        \"p\": [1, 2] # power parameter - manhattan_distance = 1 (l1);and euclidean_distance = 2 (l2)\n",
    "    },\n",
    "    \"cifar10_labels\": [\"Airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"],\n",
    "    \"fashionmnist_labels\": [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "    #based on documentation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_torch_datasets(dataiterator):\n",
    "    count = 0 #track the batches\n",
    "    all_dataset = []\n",
    "    labels = []\n",
    "    iterator = iter(dataiterator) #convert into acctual iterator\n",
    "    for batch, label_batch in iterator:\n",
    "        for (image, lab) in zip(batch, label_batch):\n",
    "            img = image.numpy()\n",
    "            img = img.transpose(1, 2, 0) #from (C, H, W) to (H, W, C)\n",
    "            img = (img * 255).astype(np.uint8) #rescale img [0, 1] to [0, 255]\n",
    "            all_dataset.append(img)\n",
    "            labels.append(lab)\n",
    "\n",
    "    return np.array(all_dataset), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(image_array):\n",
    "\n",
    "    # Calculate the histograms for each channel\n",
    "    if image_array.shape[2] == 3: # is the 3rd dim. in RGB form\n",
    "        hist = cv2.calcHist([image_array], [0, 1, 2], None, [16, 16, 16], [0, 256, 0, 256, 0, 256])\n",
    "        #3d hist. for RGb\n",
    "    else:\n",
    "        hist = cv2.calcHist([image_array], [0], None, [256], [0, 256]) #for greyscale 1D hist.\n",
    "    \n",
    "    histogram = cv2.normalize(hist, hist).flatten() #to 1D array\n",
    "\n",
    "    return histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(image_array):\n",
    "\n",
    "    if image_array.shape[2] == 3:\n",
    "        #convert to grayscale\n",
    "        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #detect distinct features with SIFT\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    #SIFT keypoints and descriptors\n",
    "    kp, des = sift.detectAndCompute(image_array, None)\n",
    "\n",
    "    return kp, des"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading & Processing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor() #transf. to tensor\n",
    "])\n",
    "\n",
    "dataset_name = config[\"dataset\"].lower()\n",
    "\n",
    "if dataset_name == \"cifar10\":\n",
    "    trainset = datasets.CIFAR10(\"./data\", download=True, train=True, transform=transform)\n",
    "    testset = datasets.CIFAR10(\"./data\", download=True, train=False, transform=transform)\n",
    "elif dataset_name == \"fashionmnist\":\n",
    "    trainset = datasets.FashionMNIST(\"./data\", download=True, train=True, transform=transform)\n",
    "    testset = datasets.FashionMNIST(\"./data\", download=True, train=False, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = process_torch_datasets(train_loader)\n",
    "test_images, test_labels = process_torch_datasets(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b6c4247f3201021b5e2665f3772d823801453daeff8bb188f999cd08d533fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
