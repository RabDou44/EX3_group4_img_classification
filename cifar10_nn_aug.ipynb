{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: CIFAR-10 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cifar10 image classification using customized Net architecture with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a deep learning pipeline using PyTorch to train a Convolutional Neural Network (CNN) on the CIFAR-10 dataset, a standard benchmark in computer vision. It begins by importing necessary PyTorch modules, setting up data augmentation techniques for training and testing datasets, and preparing the data loaders. This is crucial for enhancing the model's generalization by introducing variability in the training data, thus preventing overfitting.\n",
    "\n",
    "Then further is demonstrated how to modify a pre-trained ResNet18 model to classify images into 10 categories specific to the CIFAR-10 dataset. By freezing the pre-existing layers of the model and replacing the final fully connected layer to match the number of target classes, the snippet efficiently leverages transfer learning. This approach significantly reduces training time and computational expense by reusing learned features from a model trained on a much larger dataset. The training loop involves forward passes, loss computation, backward passes for gradient computation, and optimizer steps to update the model's weights, illustrating the entire process of training a neural network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mimis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mimis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 14.990\n",
      "[1,   200] loss: 12.055\n",
      "[1,   300] loss: 11.857\n",
      "[1,   400] loss: 12.062\n",
      "[1,   500] loss: 12.149\n",
      "[1,   600] loss: 13.187\n",
      "[1,   700] loss: 13.664\n",
      "[2,   100] loss: 13.116\n",
      "[2,   200] loss: 13.599\n",
      "[2,   300] loss: 13.004\n",
      "[2,   400] loss: 12.232\n",
      "[2,   500] loss: 12.206\n",
      "[2,   600] loss: 12.166\n",
      "[2,   700] loss: 12.395\n",
      "[3,   100] loss: 13.371\n",
      "[3,   200] loss: 12.793\n",
      "[3,   300] loss: 12.809\n",
      "[3,   400] loss: 11.556\n",
      "[3,   500] loss: 11.588\n",
      "[3,   600] loss: 12.665\n",
      "[3,   700] loss: 12.897\n",
      "[4,   100] loss: 13.905\n",
      "[4,   200] loss: 13.187\n",
      "[4,   300] loss: 12.485\n",
      "[4,   400] loss: 12.509\n",
      "[4,   500] loss: 11.955\n",
      "[4,   600] loss: 13.491\n",
      "[4,   700] loss: 13.210\n",
      "[5,   100] loss: 12.467\n",
      "[5,   200] loss: 12.940\n",
      "[5,   300] loss: 13.305\n",
      "[5,   400] loss: 13.378\n",
      "[5,   500] loss: 13.946\n",
      "[5,   600] loss: 13.061\n",
      "[5,   700] loss: 12.250\n",
      "[6,   100] loss: 12.589\n",
      "[6,   200] loss: 11.912\n",
      "[6,   300] loss: 12.813\n",
      "[6,   400] loss: 12.782\n",
      "[6,   500] loss: 11.498\n",
      "[6,   600] loss: 12.026\n",
      "[6,   700] loss: 13.172\n",
      "[7,   100] loss: 12.704\n",
      "[7,   200] loss: 11.994\n",
      "[7,   300] loss: 12.382\n",
      "[7,   400] loss: 12.936\n",
      "[7,   500] loss: 11.727\n",
      "[7,   600] loss: 12.471\n",
      "[7,   700] loss: 13.316\n",
      "[8,   100] loss: 13.908\n",
      "[8,   200] loss: 13.433\n",
      "[8,   300] loss: 12.989\n",
      "[8,   400] loss: 12.632\n",
      "[8,   500] loss: 13.133\n",
      "[8,   600] loss: 12.235\n",
      "[8,   700] loss: 12.823\n",
      "[9,   100] loss: 12.499\n",
      "[9,   200] loss: 12.594\n",
      "[9,   300] loss: 12.460\n",
      "[9,   400] loss: 12.452\n",
      "[9,   500] loss: 13.214\n",
      "[9,   600] loss: 13.406\n",
      "[9,   700] loss: 12.250\n",
      "[10,   100] loss: 13.227\n",
      "[10,   200] loss: 12.121\n",
      "[10,   300] loss: 13.092\n",
      "[10,   400] loss: 12.127\n",
      "[10,   500] loss: 12.965\n",
      "[10,   600] loss: 12.302\n",
      "[10,   700] loss: 13.175\n",
      "[11,   100] loss: 13.056\n",
      "[11,   200] loss: 12.185\n",
      "[11,   300] loss: 13.937\n",
      "[11,   400] loss: 13.195\n",
      "[11,   500] loss: 12.162\n",
      "[11,   600] loss: 13.264\n",
      "[11,   700] loss: 12.965\n",
      "[12,   100] loss: 13.847\n",
      "[12,   200] loss: 13.923\n",
      "[12,   300] loss: 13.578\n",
      "[12,   400] loss: 12.955\n",
      "[12,   500] loss: 12.230\n",
      "[12,   600] loss: 11.827\n",
      "[12,   700] loss: 11.863\n",
      "[13,   100] loss: 13.079\n",
      "[13,   200] loss: 11.709\n",
      "[13,   300] loss: 12.694\n",
      "[13,   400] loss: 13.939\n",
      "[13,   500] loss: 12.378\n",
      "[13,   600] loss: 12.379\n",
      "[13,   700] loss: 12.706\n",
      "[14,   100] loss: 13.383\n",
      "[14,   200] loss: 13.147\n",
      "[14,   300] loss: 12.711\n",
      "[14,   400] loss: 12.711\n",
      "[14,   500] loss: 11.531\n",
      "[14,   600] loss: 12.044\n",
      "[14,   700] loss: 12.882\n",
      "[15,   100] loss: 13.320\n",
      "[15,   200] loss: 13.009\n",
      "[15,   300] loss: 13.616\n",
      "[15,   400] loss: 13.243\n",
      "[15,   500] loss: 12.178\n",
      "[15,   600] loss: 12.384\n",
      "[15,   700] loss: 13.100\n",
      "[16,   100] loss: 12.557\n",
      "[16,   200] loss: 12.382\n",
      "[16,   300] loss: 13.111\n",
      "[16,   400] loss: 13.279\n",
      "[16,   500] loss: 13.515\n",
      "[16,   600] loss: 12.793\n",
      "[16,   700] loss: 11.992\n",
      "[17,   100] loss: 12.409\n",
      "[17,   200] loss: 12.656\n",
      "[17,   300] loss: 12.715\n",
      "[17,   400] loss: 12.303\n",
      "[17,   500] loss: 13.059\n",
      "[17,   600] loss: 12.724\n",
      "[17,   700] loss: 12.509\n",
      "[18,   100] loss: 12.699\n",
      "[18,   200] loss: 13.439\n",
      "[18,   300] loss: 12.899\n",
      "[18,   400] loss: 13.486\n",
      "[18,   500] loss: 13.220\n",
      "[18,   600] loss: 12.890\n",
      "[18,   700] loss: 12.113\n",
      "[19,   100] loss: 14.151\n",
      "[19,   200] loss: 12.864\n",
      "[19,   300] loss: 12.082\n",
      "[19,   400] loss: 13.744\n",
      "[19,   500] loss: 12.624\n",
      "[19,   600] loss: 13.371\n",
      "[19,   700] loss: 14.257\n",
      "[20,   100] loss: 13.687\n",
      "[20,   200] loss: 12.297\n",
      "[20,   300] loss: 12.093\n",
      "[20,   400] loss: 12.479\n",
      "[20,   500] loss: 12.470\n",
      "[20,   600] loss: 12.918\n",
      "[20,   700] loss: 13.419\n",
      "Finished Training\n",
      "Finished training in 536.693 seconds\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Finished testing in 545.218 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "# Set up data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load the pre-trained ResNet18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the pre-trained model\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer with a new one that has 10 outputs\n",
    "resnet.fc = nn.Linear(512, 10)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet.fc.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "print('Finished training in %.3f seconds' % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Finished testing in 409476.143 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = resnet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "print('Finished testing in %.3f seconds' % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5af58a9f1f579e9777827ede5d9c45decd7486a9d3dee0f84a1468c989d5713"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
